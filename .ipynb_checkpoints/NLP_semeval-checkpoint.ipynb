{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SpbYSjvvDSYJ"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6emxqpGRlQSy",
    "outputId": "39329a7f-0dee-4368-a9a0-2ca8f6ba5d34"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "llGt_owoTl6G"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# global imports\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "# global imports\n",
    "# import os\n",
    "# from google.colab import drive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wyETT5lCL0ir"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import nltk\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Up8w9MhScAD-",
    "outputId": "63eee400-98f4-4c9b-c807-3073ed11cbf6"
   },
   "outputs": [],
   "source": [
    "! pip install contractions\n",
    "!pip install -U spacy\n",
    "!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n1ep4adPbr5z",
    "outputId": "612e33d1-5e53-44de-d1b9-81636cfddd73"
   },
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ra-HYL_qrI33"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "# import ktrain\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#from ktrain.text import StandardTextPreprocessor\n",
    "import contractions\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "from gensim.parsing.preprocessing import preprocess_string\n",
    "# getting stop_words from nltk and initializing a lemmatizer\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "from sklearn.cluster import KMeans\n",
    "import time\n",
    "import pickle\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "27sYQseqa-L6"
   },
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AMV1EJPZ7gvE"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "# import cupy as cp\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "from google.colab import drive\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lsprn9xJ7lNT"
   },
   "outputs": [],
   "source": [
    "class ABSADataset(Dataset):\n",
    "    def __init__(self, texts, aspects, labels, tokenizer, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.aspects = aspects\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        aspect = self.aspects[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        encoding = self.tokenizer(text, aspect,\n",
    "                                  return_tensors='pt',\n",
    "                                  max_length=self.max_length,\n",
    "                                  truncation=True,\n",
    "                                  padding='max_length')\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            'labels': torch.tensor(label)\n",
    "        }\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # Join tokens back into string\n",
    "    clean_text = ' '.join(filtered_tokens)\n",
    "\n",
    "    return clean_text\n",
    "\n",
    "\n",
    "def clean_aspect(aspect):\n",
    "    # Convert to lowercase\n",
    "    aspect = aspect.lower()\n",
    "\n",
    "    # Remove punctuation\n",
    "    aspect = aspect.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    return aspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "apyIwaSM7wqH"
   },
   "outputs": [],
   "source": [
    "# load the dataset with labels\n",
    "\n",
    "dataset_path = '/content/drive/MyDrive/Sem4/NLP/Final project/'\n",
    "dataset_path = dataset_path + 'Restaurants_Train_v2.xml'\n",
    "dataset = []\n",
    "\n",
    "f = ET.parse(dataset_path)\n",
    "for i in f.getroot():\n",
    "  for j in i.find(\"aspectCategories\"):\n",
    "    dataset.append({\n",
    "        \"input_text\": clean_text(i.find(\"text\").text),\n",
    "        \"aspect\": clean_aspect(j.attrib['category']),\n",
    "        \"polarity\": j.attrib['polarity']\n",
    "    })\n",
    "\n",
    "bgw_vectorizer = CountVectorizer(binary=False, stop_words='english', ngram_range=(1,4))\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1,1))\n",
    "train_dataset, test_dataset = train_test_split(dataset, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7zJWrCLEc709"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "714d5nbKdM2Q",
    "outputId": "6922312c-3595-436c-a190-2055cb528521"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "_OamqGGjdP4Y",
    "outputId": "322884db-d44d-448a-ac1f-0e653444acd5"
   },
   "outputs": [],
   "source": [
    "df['input_text'][4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hGAv2zCldsou"
   },
   "source": [
    "# Getting aspect nouns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZjLwFfbNdekt"
   },
   "outputs": [],
   "source": [
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "\n",
    "# Define a function to extract nouns (aspects) from a review\n",
    "def extract_nouns_from_review(review, nlp):\n",
    "    # Process the review using spaCy pipeline\n",
    "    doc = nlp(review)\n",
    "    # Extract nouns from the review\n",
    "    nouns = [token.text for token in doc if token.pos_ == \"NOUN\"]\n",
    "    return list(set(nouns))\n",
    "\n",
    "# Extract nouns (aspects) for each review and add them as a new column to the DataFrame\n",
    "df['extracted_nouns'] = df['input_text'].apply(lambda review: extract_nouns_from_review(review, nlp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "qquA-r_fdesN",
    "outputId": "a3a67f43-e3ef-44a5-fb9f-74232340aedf"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "909QARbwvEtg",
    "outputId": "dd109412-4fc8-40c0-9b2a-2b4e984f2036"
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xypK91S7uDIP"
   },
   "outputs": [],
   "source": [
    "df = df[~(df['input_text']=='')] #removing blank reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CNuowRUNvGcZ"
   },
   "outputs": [],
   "source": [
    "df = df[df['input_text'].apply(lambda x: len(nltk.word_tokenize(x)))>1] #removing single worded reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dkrPl8MqtLhD",
    "outputId": "127352f4-c8e0-4f9d-e4c0-c79fc162f972"
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xc28EYPNvI5b"
   },
   "outputs": [],
   "source": [
    "df.to_csv(dataset_path+'semeval_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tk1zcROCvV_P"
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv(dataset_path+'semeval_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TzOTXmfIva2b",
    "outputId": "abf434ab-e59a-4c42-b828-02915c09e525"
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GLlnJ3Gaeh41"
   },
   "source": [
    "# Getting aspect categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WBrV9ocsefFV"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# Load spaCy model with GloVe vectors\n",
    "#!python -m spacy download en_core_web_lg #run this for every new session\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# Define aspects and their corresponding words\n",
    "aspects = {\n",
    "    'food': 'food delicious tasty delicious flavor meal cuisine chef portion seasoned authentic',\n",
    "    'service': 'service staff friendly helpful attentive prompt',\n",
    "    'ambience': 'ambience ambiance atmosphere decor decoration cozy elegant modern chic view clean dirty dusty',\n",
    "    'price': 'price value affordable expensive budget cost cheap',\n",
    "    # 'anecdotesmiscellaneous': 'location parking cleanliness presentation variety'\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "# Define a function to compute aspect embeddings\n",
    "def compute_aspect_embeddings(aspects, nlp):\n",
    "    aspect_embeddings = {}\n",
    "    for aspect, words in aspects.items():\n",
    "        # Tokenize and average the word vectors for each aspect\n",
    "        tokens = nlp(words)\n",
    "        aspect_vectors = [token.vector for token in tokens if token.has_vector]\n",
    "        aspect_embeddings[aspect] = np.mean(aspect_vectors, axis=0)\n",
    "    return aspect_embeddings\n",
    "\n",
    "# Define a function to compute review embeddings\n",
    "def compute_review_embedding(review, nlp):\n",
    "    # Process the review using spaCy pipeline\n",
    "    doc = nlp(review)\n",
    "    sentence_embeddings = []\n",
    "    for sent in doc.sents:\n",
    "        # Compute sentence embedding by averaging word vectors\n",
    "        sent_vector = np.mean([word.vector for word in sent if word.has_vector], axis=0)\n",
    "        sentence_embeddings.append(sent_vector)\n",
    "    # Replace new words with zeros and compute review embedding\n",
    "    review_embedding = np.mean(sentence_embeddings, axis=0)\n",
    "    return np.nan_to_num(review_embedding)\n",
    "\n",
    "\n",
    "def compute_sentence_embedding(sentence, nlp):\n",
    "    # Process the sentence using spaCy pipeline\n",
    "    doc = nlp(sentence)\n",
    "    # Compute sentence embedding by averaging word vectors\n",
    "    sent_vector = np.mean([word.vector for word in doc if word.has_vector], axis=0)\n",
    "    # Replace NaN values with zeros\n",
    "    return np.nan_to_num(sent_vector)\n",
    "\n",
    "\n",
    "# Compute semantic similarity between review sentences and aspect embeddings\n",
    "def compute_semantic_similarity(review_sentences, aspect_embeddings):\n",
    "    aspect_names = list(aspect_embeddings.keys())\n",
    "    similarity_scores = []\n",
    "    for sentence in review_sentences:\n",
    "        sentence_vector = compute_review_embedding(sentence, nlp)\n",
    "        sentence_vector = sentence_vector.reshape(1, -1)  # Reshape to match cosine_similarity function input\n",
    "        sentence_similarity = []\n",
    "        for aspect_name, aspect_vector in aspect_embeddings.items():\n",
    "            aspect_vector = aspect_vector.reshape(1, -1)  # Reshape to match cosine_similarity function input\n",
    "            similarity = cosine_similarity(sentence_vector, aspect_vector)[0][0]\n",
    "            sentence_similarity.append(similarity)\n",
    "        similarity_scores.append({aspect_name: score for aspect_name, score in zip(aspect_names, sentence_similarity)})\n",
    "    return similarity_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7-AzrzO8efc1"
   },
   "outputs": [],
   "source": [
    "# review_sentences = df['input_text'][0:100].to_list() --working fine\n",
    "\n",
    "# review_sentences = df['input_text']\n",
    "\n",
    "review_sentences = df['input_text']\n",
    "\n",
    "#for each review in 'cleaned_text' and get a list of aspects\n",
    "\n",
    "# Compute aspect embeddings\n",
    "aspect_embeddings = compute_aspect_embeddings(aspects, nlp)\n",
    "\n",
    "# Compute semantic similarity\n",
    "semantic_similarity_scores = compute_semantic_similarity(review_sentences, aspect_embeddings)\n",
    "\n",
    "# Choose the aspect with the highest similarity score for each sentence\n",
    "chosen_aspects = [max(scores, key=scores.get) for scores in semantic_similarity_scores]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jwMsRKuOgVTB",
    "outputId": "5615e3a9-9608-4a77-f3bb-83686c397c52"
   },
   "outputs": [],
   "source": [
    "# Output the chosen aspects for each sentence\n",
    "for sentence, aspect in zip(review_sentences, chosen_aspects):\n",
    "    print(\"Sentence:\", sentence)\n",
    "    print(\"Chosen Aspect:\", aspect)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YZ3V-6M8hjC4"
   },
   "outputs": [],
   "source": [
    "df['aspect_predicted'] = chosen_aspects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "JvcFUjFwhjNS",
    "outputId": "8670cdd5-ac0d-4e24-c15a-5ca829a4ee19"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SpevKlGtyYeg"
   },
   "source": [
    "# Accuracy check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hdG-GnPJnfPQ"
   },
   "outputs": [],
   "source": [
    "df2 = df[df['aspect'] != \"anecdotesmiscellaneous\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f-vVvux1hjVA"
   },
   "outputs": [],
   "source": [
    "cnt = (df2['aspect'] == df2['aspect_predicted']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RIU523medGnG",
    "outputId": "b233e41a-c2ef-414b-c2a8-e51c876f8d18"
   },
   "outputs": [],
   "source": [
    "cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4A1ZT5FAdGkv",
    "outputId": "4d3555c9-f3cb-4700-e3ae-ef0289c6a31b"
   },
   "outputs": [],
   "source": [
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QsbV4mVZdGYY",
    "outputId": "360b669d-48cd-4518-c035-424614641f5e"
   },
   "outputs": [],
   "source": [
    "1318/2033"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qKlXgZo7xo5c"
   },
   "source": [
    "# 65% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BE4Tvg8G8vgm"
   },
   "outputs": [],
   "source": [
    "import ngram_nb\n",
    "model = ngram_nb.Ngram_NB(3)\n",
    "\n",
    "# model.train(df2, text_col = \"input_text\", category_col = \"aspect\")\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9yyQkZ_4dGSe"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
