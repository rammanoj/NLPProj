{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "llGt_owoTl6G"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\anura\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# global imports\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "# from google.colab import drive\n",
    "import json\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 2,
=======
   "execution_count": 5,
>>>>>>> 4c1e650 (First round of ngram naive bayes (imports broken))
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "woU4vDoLZ6Z3",
    "outputId": "6701ad2a-604e-41e1-dd31-5889265d9675"
   },
<<<<<<< HEAD
   "outputs": [],
   "source": [
    "# load the dataset to the colab\n",
    "\n",
    "# drive.mount('/content/drive')\n",
    "# dataset_path = '/content/drive/MyDrive/yelp_dataset/'\n",
    "\n",
    "# reviews = dataset_path + \"yelp_academic_dataset_review.json\"\n",
    "# business = dataset_path + \"yelp_academic_dataset_business.json\"\n",
    "\n",
    "\n",
    "# dataset = dataset_path + \"yelp_data.json\"\n",
    "# data_cleaned = dataset_path + 'yelp_cleaned.json'\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "# contraction_mapping = {\n",
    "#     \"ain't\": \"is not\",\n",
    "#     \"aren't\": \"are not\",\n",
    "#     \"can't\": \"cannot\",\n",
    "#     \"could've\": \"could have\",\n",
    "#     \"couldn't\": \"could not\",\n",
    "#     \"didn't\": \"did not\",\n",
    "#     \"doesn't\": \"does not\",\n",
    "#     \"don't\": \"do not\",\n",
    "#     \"gonna\": \"going to\",\n",
    "#     \"gotta\": \"got to\",\n",
    "#     \"hadn't\": \"had not\",\n",
    "#     \"hasn't\": \"has not\",\n",
    "#     \"haven't\": \"have not\",\n",
    "#     \"he'd\": \"he would\",\n",
    "#     \"he'll\": \"he will\",\n",
    "#     \"he's\": \"he is\",\n",
    "#     \"how'd\": \"how did\",\n",
    "#     \"how'll\": \"how will\",\n",
    "#     \"how's\": \"how is\",\n",
    "#     \"I'd\": \"I would\",\n",
    "#     \"I'll\": \"I will\",\n",
    "#     \"I'm\": \"I am\",\n",
    "#     \"I've\": \"I have\",\n",
    "#     \"isn't\": \"is not\",\n",
    "#     \"it'd\": \"it would\",\n",
    "#     \"it'll\": \"it will\",\n",
    "#     \"it's\": \"it is\",\n",
    "#     \"let's\": \"let us\",\n",
    "#     \"might've\": \"might have\",\n",
    "#     \"must've\": \"must have\",\n",
    "#     \"mustn't\": \"must not\",\n",
    "#     \"shan't\": \"shall not\",\n",
    "#     \"she'd\": \"she would\",\n",
    "#     \"she'll\": \"she will\",\n",
    "#     \"she's\": \"she is\",\n",
    "#     \"should've\": \"should have\",\n",
    "#     \"shouldn't\": \"should not\",\n",
    "#     \"that'll\": \"that will\",\n",
    "#     \"that's\": \"that is\",\n",
    "#     \"there's\": \"there is\",\n",
    "#     \"they'd\": \"they would\",\n",
    "#     \"they'll\": \"they will\",\n",
    "#     \"they're\": \"they are\",\n",
    "#     \"they've\": \"they have\",\n",
    "#     \"wasn't\": \"was not\",\n",
    "#     \"we'd\": \"we would\",\n",
    "#     \"we'll\": \"we will\",\n",
    "#     \"we're\": \"we are\",\n",
    "#     \"we've\": \"we have\",\n",
    "#     \"weren't\": \"were not\",\n",
    "#     \"what'd\": \"what did\",\n",
    "#     \"what'll\": \"what will\",\n",
    "#     \"what're\": \"what are\",\n",
    "#     \"what's\": \"what is\",\n",
    "#     \"what've\": \"what have\",\n",
    "#     \"when's\": \"when is\",\n",
    "#     \"where'd\": \"where did\",\n",
    "#     \"where'll\": \"where will\",\n",
    "#     \"where's\": \"where is\",\n",
    "#     \"who'd\": \"who did\",\n",
    "#     \"who'll\": \"who will\",\n",
    "#     \"who's\": \"who is\",\n",
    "#     \"who've\": \"who have\",\n",
    "#     \"why'd\": \"why did\",\n",
    "#     \"why'll\": \"why will\",\n",
    "#     \"why's\": \"why is\",\n",
    "#     \"won't\": \"will not\",\n",
    "#     \"would've\": \"would have\",\n",
    "#     \"wouldn't\": \"would not\",\n",
    "#     \"you'd\": \"you would\",\n",
    "#     \"you'll\": \"you will\",\n",
    "#     \"you're\": \"you are\",\n",
    "#     \"you've\": \"you have\"\n",
    "# }"
=======
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'drive' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# load the dataset to the colab\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m dataset_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive/MyDrive/yelp_dataset/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      6\u001b[0m reviews \u001b[38;5;241m=\u001b[39m dataset_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myelp_academic_dataset_review.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'drive' is not defined"
     ]
    }
   ],
   "source": [
    "# load the dataset to the colab\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "dataset_path = '/content/drive/MyDrive/yelp_dataset/'\n",
    "\n",
    "reviews = dataset_path + \"yelp_academic_dataset_review.json\"\n",
    "business = dataset_path + \"yelp_academic_dataset_business.json\"\n",
    "\n",
    "\n",
    "dataset = dataset_path + \"yelp_data.json\"\n",
    "data_cleaned = dataset_path + 'yelp_cleaned.json'\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "contraction_mapping = {\n",
    "    \"ain't\": \"is not\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"can't\": \"cannot\",\n",
    "    \"could've\": \"could have\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"gonna\": \"going to\",\n",
    "    \"gotta\": \"got to\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"he'd\": \"he would\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"how'd\": \"how did\",\n",
    "    \"how'll\": \"how will\",\n",
    "    \"how's\": \"how is\",\n",
    "    \"I'd\": \"I would\",\n",
    "    \"I'll\": \"I will\",\n",
    "    \"I'm\": \"I am\",\n",
    "    \"I've\": \"I have\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"it'd\": \"it would\",\n",
    "    \"it'll\": \"it will\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"might've\": \"might have\",\n",
    "    \"must've\": \"must have\",\n",
    "    \"mustn't\": \"must not\",\n",
    "    \"shan't\": \"shall not\",\n",
    "    \"she'd\": \"she would\",\n",
    "    \"she'll\": \"she will\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"should've\": \"should have\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"that'll\": \"that will\",\n",
    "    \"that's\": \"that is\",\n",
    "    \"there's\": \"there is\",\n",
    "    \"they'd\": \"they would\",\n",
    "    \"they'll\": \"they will\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"we'd\": \"we would\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"what'd\": \"what did\",\n",
    "    \"what'll\": \"what will\",\n",
    "    \"what're\": \"what are\",\n",
    "    \"what's\": \"what is\",\n",
    "    \"what've\": \"what have\",\n",
    "    \"when's\": \"when is\",\n",
    "    \"where'd\": \"where did\",\n",
    "    \"where'll\": \"where will\",\n",
    "    \"where's\": \"where is\",\n",
    "    \"who'd\": \"who did\",\n",
    "    \"who'll\": \"who will\",\n",
    "    \"who's\": \"who is\",\n",
    "    \"who've\": \"who have\",\n",
    "    \"why'd\": \"why did\",\n",
    "    \"why'll\": \"why will\",\n",
    "    \"why's\": \"why is\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"would've\": \"would have\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"you'd\": \"you would\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"you've\": \"you have\"\n",
    "}"
>>>>>>> 4c1e650 (First round of ngram naive bayes (imports broken))
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
=======
   "execution_count": null,
>>>>>>> 4c1e650 (First round of ngram naive bayes (imports broken))
   "metadata": {
    "id": "yKYaySv0iuME"
   },
   "outputs": [],
   "source": [
    "# **Note:** Only for one-time execution\n",
    "\n",
    "# Condense the dataset to only Restuarents and Food stalls\n",
    "\n",
    "\n",
<<<<<<< HEAD
    "# food_related_items = ['Chicken Wings', 'Bakeries', 'Creperies', 'Asian Fusion', 'Dumplings', 'Chocolatiers & Shops', 'Indian', 'Hot Tub & Pool', 'Ethiopian', 'Hawaiian', 'Poke', 'Soup', 'Vietnamese', 'Waffles', 'Falafel', 'Vegetarian', 'Cheesesteaks', 'Caribbean', 'Pita', 'Sushi Bars', 'Soup', 'Halal', 'Turkish', 'Chinese', 'Japanese Curry', 'Breweries', 'Taiwanese', 'Russian', 'Thai', 'Afghan', 'Tex-Mex', 'Iberian', 'Peruvian', 'Salvadoran', 'Laotian', 'Korean', 'Mexican', 'Dim Sum', 'Hakka', 'Venezuelan', 'Pakistani', 'Malaysian', 'Brazilian', 'Colombian', 'Cajun/Creole', 'Bubble Tea', 'Kebab', 'Trinidadian', 'Cambodian', 'Japanese', 'Tapas Bars', 'Tapas/Small Plates', 'Greek', 'restaurants', 'food']\n",
    "\n",
    "# food_related_business_ids = set()\n",
    "# with open(business, \"r\") as f:\n",
    "#   for i in f:\n",
    "#     json_data = json.loads(i)\n",
    "#     if json_data[\"categories\"] != None:\n",
    "#       for j in food_related_items:\n",
    "#         if j in json_data[\"categories\"]:\n",
    "#           food_related_business_ids.add(json_data[\"business_id\"])\n",
    "\n",
    "\n",
    "# def filter_specific_ids(file_name, out_file_name):\n",
    "#     out_file = open(out_file_name, \"w+\")\n",
    "#     start = True\n",
    "#     with open(file_name, \"r\") as f:\n",
    "#         out_file.write('[')\n",
    "#         for i in f:\n",
    "#             if json.loads(i)[\"business_id\"] in food_related_business_ids:\n",
    "#               if start:\n",
    "#                   start = False\n",
    "#               else:\n",
    "#                   out_file.write(\",\")\n",
    "#               data = json.loads(i)\n",
    "#               del data[\"review_id\"]\n",
    "#               del data[\"user_id\"]\n",
    "#               del data[\"business_id\"]\n",
    "#               del data[\"date\"]\n",
    "#               out_file.write(json.dumps(data))\n",
    "#     out_file.write(']')\n",
    "#     out_file.close()\n",
    "\n",
    "\n",
    "# filter_specific_ids(reviews, dataset)"
=======
    "food_related_items = ['Chicken Wings', 'Bakeries', 'Creperies', 'Asian Fusion', 'Dumplings', 'Chocolatiers & Shops', 'Indian', 'Hot Tub & Pool', 'Ethiopian', 'Hawaiian', 'Poke', 'Soup', 'Vietnamese', 'Waffles', 'Falafel', 'Vegetarian', 'Cheesesteaks', 'Caribbean', 'Pita', 'Sushi Bars', 'Soup', 'Halal', 'Turkish', 'Chinese', 'Japanese Curry', 'Breweries', 'Taiwanese', 'Russian', 'Thai', 'Afghan', 'Tex-Mex', 'Iberian', 'Peruvian', 'Salvadoran', 'Laotian', 'Korean', 'Mexican', 'Dim Sum', 'Hakka', 'Venezuelan', 'Pakistani', 'Malaysian', 'Brazilian', 'Colombian', 'Cajun/Creole', 'Bubble Tea', 'Kebab', 'Trinidadian', 'Cambodian', 'Japanese', 'Tapas Bars', 'Tapas/Small Plates', 'Greek', 'restaurants', 'food']\n",
    "\n",
    "food_related_business_ids = set()\n",
    "with open(business, \"r\") as f:\n",
    "  for i in f:\n",
    "    json_data = json.loads(i)\n",
    "    if json_data[\"categories\"] != None:\n",
    "      for j in food_related_items:\n",
    "        if j in json_data[\"categories\"]:\n",
    "          food_related_business_ids.add(json_data[\"business_id\"])\n",
    "\n",
    "\n",
    "def filter_specific_ids(file_name, out_file_name):\n",
    "    out_file = open(out_file_name, \"w+\")\n",
    "    start = True\n",
    "    with open(file_name, \"r\") as f:\n",
    "        out_file.write('[')\n",
    "        for i in f:\n",
    "            if json.loads(i)[\"business_id\"] in food_related_business_ids:\n",
    "              if start:\n",
    "                  start = False\n",
    "              else:\n",
    "                  out_file.write(\",\")\n",
    "              data = json.loads(i)\n",
    "              del data[\"review_id\"]\n",
    "              del data[\"user_id\"]\n",
    "              del data[\"business_id\"]\n",
    "              del data[\"date\"]\n",
    "              out_file.write(json.dumps(data))\n",
    "    out_file.write(']')\n",
    "    out_file.close()\n",
    "\n",
    "\n",
    "filter_specific_ids(reviews, dataset)"
>>>>>>> 4c1e650 (First round of ngram naive bayes (imports broken))
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 4,
   "metadata": {
    "id": "RHPPmyjfhzPR"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# **Note:** Only for one-time execution\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# cleanse the data\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m dataframe \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_json(dataset)\n\u001b[0;32m      5\u001b[0m stop_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(nltk\u001b[38;5;241m.\u001b[39mcorpus\u001b[38;5;241m.\u001b[39mstopwords\u001b[38;5;241m.\u001b[39mwords(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m      6\u001b[0m lemmatizer \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mstem\u001b[38;5;241m.\u001b[39mWordNetLemmatizer()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# # **Note:** Only for one-time execution\n",
    "\n",
    "# # cleanse the data\n",
    "# dataframe = pd.read_json(dataset)\n",
    "# stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "# lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "# def cleanse_data(d):\n",
    "\n",
    "#   #convert to lower case\n",
    "#   o = d.lower()\n",
    "\n",
    "#   # replace contractions with full forms\n",
    "#   for j in contraction_mapping.keys():\n",
    "#     o = o.replace(j, contraction_mapping[j])\n",
    "\n",
    "#   # replace extra white spaces, special characters\n",
    "#   o = re.sub(r'\\s+', ' ', o)\n",
    "#   o = re.sub(r'[^a-zA-Z0-9\\s]', '', o)\n",
    "\n",
    "#   # remove all new lines\n",
    "#   o = o.replace(\"\\n\", \"\")\n",
    "\n",
    "#   # remove stop words and lemmatize the tokens\n",
    "#   tokens = o.split()\n",
    "#   cleaned_tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
    "#   o = ' '.join(cleaned_tokens)\n",
    "\n",
    "#   # remove punctuation\n",
    "#   o = o.translate(str.maketrans('', '', string.punctuation))\n",
    "#   return o\n",
    "\n",
    "# dataframe[\"text\"] = dataframe[\"text\"].apply(cleanse_data)\n",
    "# dataframe.to_json(data_cleaned)"
=======
   "execution_count": null,
   "metadata": {
    "id": "RHPPmyjfhzPR"
   },
   "outputs": [],
   "source": [
    "# **Note:** Only for one-time execution\n",
    "\n",
    "# cleanse the data\n",
    "dataframe = pd.read_json(dataset)\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def cleanse_data(d):\n",
    "\n",
    "  #convert to lower case\n",
    "  o = d.lower()\n",
    "\n",
    "  # replace contractions with full forms\n",
    "  for j in contraction_mapping.keys():\n",
    "    o = o.replace(j, contraction_mapping[j])\n",
    "\n",
    "  # replace extra white spaces, special characters\n",
    "  o = re.sub(r'\\s+', ' ', o)\n",
    "  o = re.sub(r'[^a-zA-Z0-9\\s]', '', o)\n",
    "\n",
    "  # remove all new lines\n",
    "  o = o.replace(\"\\n\", \"\")\n",
    "\n",
    "  # remove stop words and lemmatize the tokens\n",
    "  tokens = o.split()\n",
    "  cleaned_tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
    "  o = ' '.join(cleaned_tokens)\n",
    "\n",
    "  # remove punctuation\n",
    "  o = o.translate(str.maketrans('', '', string.punctuation))\n",
    "  return o\n",
    "\n",
    "dataframe[\"text\"] = dataframe[\"text\"].apply(cleanse_data)\n",
    "dataframe.to_json(data_cleaned)"
>>>>>>> 4c1e650 (First round of ngram naive bayes (imports broken))
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "esaM9EX3tzry",
    "outputId": "14b384c9-11e6-4bf9-ea73-dccf24289e3a"
   },
   "outputs": [],
   "source": [
    "# Run to load the dataset\n",
    "\n",
    "json_data = pd.read_json(\"./yelp_cleaned.json\")\n",
    "json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pZkIGKRtCB2p"
   },
   "outputs": [],
   "source": [
    "## Topic modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aspect Extraction\n",
    "# https://www.enjoyalgorithms.com/blog/aspect-base-sentiment-analysis-in-python\n",
    "# run the command below once\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 6,
>>>>>>> 4c1e650 (First round of ngram naive bayes (imports broken))
   "metadata": {},
   "outputs": [],
   "source": [
    "xpath_expression1 = \" //sentence\"\n",
    "xpath_expression2 = \"//sentence/aspectCategories/aspectCategory[1]\"\n",
    "df = pd.read_xml('Restaurants_Train_v2.xml', xpath = xpath_expression1)\n",
    "df2 = pd.read_xml('Restaurants_Train_v2.xml', xpath = xpath_expression2)\n",
    "# Extract category attribute from aspectCategories\n",
    "# df['category'] = df['aspectCategories'].apply(lambda x: x['aspectCategory']['category'])\n",
    "\n",
    "# Drop the aspectCategories column as it's no longer needed\n",
    "# df.drop(columns='aspectCategories', inplace=True)\n",
    "\n",
    "final_df = df.merge(df2, left_index=True, right_index=True)\n",
    "final_df.dropna(axis = 1, inplace=True)\n",
    "categories = final_df['category'].unique()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
=======
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "SENTENCE_BEGIN = \"<s>\"\n",
    "SENTENCE_END = \"</s>\"\n",
    "UNK = \"<UNK>\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_line(line: str, ngram: int,\n",
    "                  by_char: bool = False,\n",
    "                  sentence_begin: str = SENTENCE_BEGIN,\n",
    "                  sentence_end: str = SENTENCE_END):\n",
    "    \"\"\"\n",
    "  Tokenize a single string. Glue on the appropriate number of \n",
    "  sentence begin tokens and sentence end tokens (ngram - 1), except\n",
    "  for the case when ngram == 1, when there will be one sentence begin\n",
    "  and one sentence end token.\n",
    "  Args:\n",
    "    line (str): text to tokenize\n",
    "    ngram (int): ngram preparation number\n",
    "    by_char (bool): default value True, if True, tokenize by character, if\n",
    "      False, tokenize by whitespace\n",
    "    sentence_begin (str): sentence begin token value\n",
    "    sentence_end (str): sentence end token value\n",
    "\n",
    "  Returns:\n",
    "    list of strings - a single line tokenized\n",
    "  \"\"\"\n",
    "    for p in string.punctuation:\n",
    "        line = line.replace(p, '')\n",
    "    # PROVIDED\n",
    "    inner_pieces = None\n",
    "    if by_char:\n",
    "        inner_pieces = list(line)\n",
    "    else:\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        # otherwise split on white space and ignore stopwords\n",
    "        inner_pieces = [w.lower() for w in line.split() if w.lower() not in stop_words]\n",
    "\n",
    "    if ngram == 1:\n",
    "        tokens = [sentence_begin] + inner_pieces + [sentence_end]\n",
    "    else:\n",
    "        tokens = ([sentence_begin] * (ngram - 1)) + inner_pieces + ([sentence_end] * (ngram - 1))\n",
    "    # always count the unigrams\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "service 410\n",
      "food 1157\n",
      "anecdotes/miscellaneous 1081\n",
      "ambience 260\n",
      "price 133\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3121</td>\n",
       "      <td>But the staff was so horrible to us.</td>\n",
       "      <td>service</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2777</td>\n",
       "      <td>To be completely fair, the only redeeming fact...</td>\n",
       "      <td>food</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1634</td>\n",
       "      <td>The food is uniformly exceptional, with a very...</td>\n",
       "      <td>food</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2534</td>\n",
       "      <td>Where Gabriela personaly greets you and recomm...</td>\n",
       "      <td>service</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>583</td>\n",
       "      <td>For those that go once and don't enjoy it, all...</td>\n",
       "      <td>anecdotes/miscellaneous</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3036</th>\n",
       "      <td>1063</td>\n",
       "      <td>But that is highly forgivable.</td>\n",
       "      <td>anecdotes/miscellaneous</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3037</th>\n",
       "      <td>777</td>\n",
       "      <td>From the appetizers we ate, the dim sum and ot...</td>\n",
       "      <td>food</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3038</th>\n",
       "      <td>875</td>\n",
       "      <td>When we arrived at 6:00 PM, the restaurant was...</td>\n",
       "      <td>anecdotes/miscellaneous</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3039</th>\n",
       "      <td>671</td>\n",
       "      <td>Each table has a pot of boiling water sunken i...</td>\n",
       "      <td>food</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3040</th>\n",
       "      <td>617</td>\n",
       "      <td>I am going to the mid town location next.</td>\n",
       "      <td>anecdotes/miscellaneous</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3041 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text  \\\n",
       "0     3121               But the staff was so horrible to us.   \n",
       "1     2777  To be completely fair, the only redeeming fact...   \n",
       "2     1634  The food is uniformly exceptional, with a very...   \n",
       "3     2534  Where Gabriela personaly greets you and recomm...   \n",
       "4      583  For those that go once and don't enjoy it, all...   \n",
       "...    ...                                                ...   \n",
       "3036  1063                     But that is highly forgivable.   \n",
       "3037   777  From the appetizers we ate, the dim sum and ot...   \n",
       "3038   875  When we arrived at 6:00 PM, the restaurant was...   \n",
       "3039   671  Each table has a pot of boiling water sunken i...   \n",
       "3040   617          I am going to the mid town location next.   \n",
       "\n",
       "                     category  polarity  \n",
       "0                     service  negative  \n",
       "1                        food  positive  \n",
       "2                        food  positive  \n",
       "3                     service  positive  \n",
       "4     anecdotes/miscellaneous  positive  \n",
       "...                       ...       ...  \n",
       "3036  anecdotes/miscellaneous  positive  \n",
       "3037                     food  positive  \n",
       "3038  anecdotes/miscellaneous   neutral  \n",
       "3039                     food   neutral  \n",
       "3040  anecdotes/miscellaneous   neutral  \n",
       "\n",
       "[3041 rows x 4 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
>>>>>>> 4c1e650 (First round of ngram naive bayes (imports broken))
    "for category in categories:\n",
    "    print(category, len(final_df[final_df['category'] == category]))\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ngram_nb import Ngram_NB \n",
    "import importlib"
=======
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>',\n",
       " 'but',\n",
       " 'the',\n",
       " 'staff',\n",
       " 'was',\n",
       " 'so',\n",
       " 'horrible',\n",
       " 'to',\n",
       " 'us',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'to',\n",
       " 'be',\n",
       " 'completely',\n",
       " 'fair',\n",
       " 'the',\n",
       " 'only',\n",
       " 'redeeming',\n",
       " 'factor',\n",
       " 'was',\n",
       " 'the',\n",
       " 'food',\n",
       " 'which',\n",
       " 'was',\n",
       " 'above',\n",
       " 'average',\n",
       " 'but',\n",
       " 'couldnt',\n",
       " 'make',\n",
       " 'up',\n",
       " 'for',\n",
       " 'all',\n",
       " 'the',\n",
       " 'other',\n",
       " 'deficiencies',\n",
       " 'of',\n",
       " 'teodora',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'the',\n",
       " 'food',\n",
       " 'is',\n",
       " 'uniformly',\n",
       " 'exceptional',\n",
       " 'with',\n",
       " 'a',\n",
       " 'very',\n",
       " 'capable',\n",
       " 'kitchen',\n",
       " 'which',\n",
       " 'will',\n",
       " 'proudly',\n",
       " 'whip',\n",
       " 'up',\n",
       " 'whatever',\n",
       " 'you',\n",
       " 'feel',\n",
       " 'like',\n",
       " 'eating',\n",
       " 'whether',\n",
       " 'its',\n",
       " 'on',\n",
       " 'the',\n",
       " 'menu',\n",
       " 'or',\n",
       " 'not',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'where',\n",
       " 'gabriela',\n",
       " 'personaly',\n",
       " 'greets',\n",
       " 'you',\n",
       " 'and',\n",
       " 'recommends',\n",
       " 'you',\n",
       " 'what',\n",
       " 'to',\n",
       " 'eat',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'for',\n",
       " 'those',\n",
       " 'that',\n",
       " 'go',\n",
       " 'once',\n",
       " 'and',\n",
       " 'dont',\n",
       " 'enjoy',\n",
       " 'it',\n",
       " 'all',\n",
       " 'i',\n",
       " 'can',\n",
       " 'say',\n",
       " 'is',\n",
       " 'that',\n",
       " 'they',\n",
       " 'just',\n",
       " 'dont',\n",
       " 'get',\n",
       " 'it',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'not',\n",
       " 'only',\n",
       " 'was',\n",
       " 'the',\n",
       " 'food',\n",
       " 'outstanding',\n",
       " 'but',\n",
       " 'the',\n",
       " 'little',\n",
       " 'perks',\n",
       " 'were',\n",
       " 'great',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'it',\n",
       " 'is',\n",
       " 'very',\n",
       " 'overpriced',\n",
       " 'and',\n",
       " 'not',\n",
       " 'very',\n",
       " 'tasty',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'our',\n",
       " 'agreed',\n",
       " 'favorite',\n",
       " 'is',\n",
       " 'the',\n",
       " 'orrechiete',\n",
       " 'with',\n",
       " 'sausage',\n",
       " 'and',\n",
       " 'chicken',\n",
       " 'usually',\n",
       " 'the',\n",
       " 'waiters',\n",
       " 'are',\n",
       " 'kind',\n",
       " 'enough',\n",
       " 'to',\n",
       " 'split',\n",
       " 'the',\n",
       " 'dish',\n",
       " 'in',\n",
       " 'half',\n",
       " 'so',\n",
       " 'you',\n",
       " 'get',\n",
       " 'to',\n",
       " 'sample',\n",
       " 'both',\n",
       " 'meats',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'the',\n",
       " 'bagels',\n",
       " 'have',\n",
       " 'an',\n",
       " 'outstanding',\n",
       " 'taste',\n",
       " 'with',\n",
       " 'a',\n",
       " 'terrific',\n",
       " 'texture',\n",
       " 'both',\n",
       " 'chewy',\n",
       " 'yet',\n",
       " 'not',\n",
       " 'gummy',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'nevertheless',\n",
       " 'the',\n",
       " 'food',\n",
       " 'itself',\n",
       " 'is',\n",
       " 'pretty',\n",
       " 'good',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'they',\n",
       " 'did',\n",
       " 'not',\n",
       " 'have',\n",
       " 'mayonnaise',\n",
       " 'forgot',\n",
       " 'our',\n",
       " 'toast',\n",
       " 'left',\n",
       " 'out',\n",
       " 'ingredients',\n",
       " 'ie',\n",
       " 'cheese',\n",
       " 'in',\n",
       " 'an',\n",
       " 'omelet',\n",
       " 'below',\n",
       " 'hot',\n",
       " 'temperatures',\n",
       " 'and',\n",
       " 'the',\n",
       " 'bacon',\n",
       " 'was',\n",
       " 'so',\n",
       " 'over',\n",
       " 'cooked',\n",
       " 'it',\n",
       " 'crumbled',\n",
       " 'on',\n",
       " 'the',\n",
       " 'plate',\n",
       " 'when',\n",
       " 'you',\n",
       " 'touched',\n",
       " 'it',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'it',\n",
       " 'took',\n",
       " 'half',\n",
       " 'an',\n",
       " 'hour',\n",
       " 'to',\n",
       " 'get',\n",
       " 'our',\n",
       " 'check',\n",
       " 'which',\n",
       " 'was',\n",
       " 'perfect',\n",
       " 'since',\n",
       " 'we',\n",
       " 'could',\n",
       " 'sit',\n",
       " 'have',\n",
       " 'drinks',\n",
       " 'and',\n",
       " 'talk',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'the',\n",
       " 'design',\n",
       " 'and',\n",
       " 'atmosphere',\n",
       " 'is',\n",
       " 'just',\n",
       " 'as',\n",
       " 'good',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'he',\n",
       " 'has',\n",
       " 'visited',\n",
       " 'thailand',\n",
       " 'and',\n",
       " 'is',\n",
       " 'quite',\n",
       " 'expert',\n",
       " 'on',\n",
       " 'the',\n",
       " 'cuisine',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'i',\n",
       " 'would',\n",
       " 'definitely',\n",
       " 'recommend',\n",
       " 'marys',\n",
       " 'and',\n",
       " 'am',\n",
       " 'making',\n",
       " 'it',\n",
       " 'one',\n",
       " 'of',\n",
       " 'my',\n",
       " 'regular',\n",
       " 'neighborhood',\n",
       " 'haunts',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'the',\n",
       " 'pizza',\n",
       " 'is',\n",
       " 'the',\n",
       " 'best',\n",
       " 'if',\n",
       " 'you',\n",
       " 'like',\n",
       " 'thin',\n",
       " 'crusted',\n",
       " 'pizza',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'my',\n",
       " 'girlfriend',\n",
       " 'and',\n",
       " 'i',\n",
       " 'stumbled',\n",
       " 'onto',\n",
       " 'this',\n",
       " 'hopping',\n",
       " 'place',\n",
       " 'the',\n",
       " 'other',\n",
       " 'night',\n",
       " 'and',\n",
       " 'had',\n",
       " 'a',\n",
       " 'great',\n",
       " 'time',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'he',\n",
       " 'cant',\n",
       " 'help',\n",
       " 'theyre',\n",
       " 'bought',\n",
       " 'up',\n",
       " 'so',\n",
       " 'fast',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'secondly',\n",
       " 'on',\n",
       " 'this',\n",
       " 'night',\n",
       " 'the',\n",
       " 'place',\n",
       " 'was',\n",
       " 'overwhelmed',\n",
       " 'by',\n",
       " 'upper',\n",
       " 'east',\n",
       " 'side',\n",
       " 'ladies',\n",
       " 'perfume',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'i',\n",
       " 'have',\n",
       " 'eaten',\n",
       " 'here',\n",
       " 'a',\n",
       " 'handful',\n",
       " 'of',\n",
       " 'times',\n",
       " 'for',\n",
       " 'no',\n",
       " 'reason',\n",
       " 'besides',\n",
       " 'sheer',\n",
       " 'convenience',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'obviously',\n",
       " 'run',\n",
       " 'by',\n",
       " 'folks',\n",
       " 'who',\n",
       " 'know',\n",
       " 'a',\n",
       " 'pie',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'we',\n",
       " 'were',\n",
       " 'very',\n",
       " 'disappointed',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'it',\n",
       " 'is',\n",
       " 'definitely',\n",
       " 'special',\n",
       " 'and',\n",
       " 'affordable',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'i',\n",
       " 'really',\n",
       " 'liked',\n",
       " 'this',\n",
       " 'place',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'i',\n",
       " 'will',\n",
       " 'be',\n",
       " 'going',\n",
       " 'back',\n",
       " 'very',\n",
       " 'soon',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'all',\n",
       " 'the',\n",
       " 'money',\n",
       " 'went',\n",
       " 'into',\n",
       " 'the',\n",
       " 'interior',\n",
       " 'decoration',\n",
       " 'none',\n",
       " 'of',\n",
       " 'it',\n",
       " 'went',\n",
       " 'to',\n",
       " 'the',\n",
       " 'chefs',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'the',\n",
       " 'seats',\n",
       " 'are',\n",
       " 'uncomfortable',\n",
       " 'if',\n",
       " 'you',\n",
       " 'are',\n",
       " 'sitting',\n",
       " 'against',\n",
       " 'the',\n",
       " 'wall',\n",
       " 'on',\n",
       " 'wooden',\n",
       " 'benches',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'i',\n",
       " 'asked',\n",
       " 'for',\n",
       " 'seltzer',\n",
       " 'with',\n",
       " 'lime',\n",
       " 'no',\n",
       " 'ice',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'it',\n",
       " 'was',\n",
       " 'pretty',\n",
       " 'inexpensive',\n",
       " 'too',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'will',\n",
       " 'not',\n",
       " 'be',\n",
       " 'back',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'dont',\n",
       " 'go',\n",
       " 'aloneeven',\n",
       " 'two',\n",
       " 'people',\n",
       " 'isnt',\n",
       " 'enough',\n",
       " 'for',\n",
       " 'the',\n",
       " 'whole',\n",
       " 'experience',\n",
       " 'with',\n",
       " 'pickles',\n",
       " 'and',\n",
       " 'a',\n",
       " 'selection',\n",
       " 'of',\n",
       " 'meats',\n",
       " 'and',\n",
       " 'seafoods',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'my',\n",
       " 'suggestion',\n",
       " 'is',\n",
       " 'to',\n",
       " 'eat',\n",
       " 'family',\n",
       " 'style',\n",
       " 'because',\n",
       " 'youll',\n",
       " 'want',\n",
       " 'to',\n",
       " 'try',\n",
       " 'the',\n",
       " 'other',\n",
       " 'dishes',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'best',\n",
       " 'of',\n",
       " 'all',\n",
       " 'is',\n",
       " 'the',\n",
       " 'warm',\n",
       " 'vibe',\n",
       " 'the',\n",
       " 'owner',\n",
       " 'is',\n",
       " 'super',\n",
       " 'friendly',\n",
       " 'and',\n",
       " 'service',\n",
       " 'is',\n",
       " 'fast',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'this',\n",
       " 'isnt',\n",
       " 'a',\n",
       " 'value',\n",
       " 'joint',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'faans',\n",
       " 'got',\n",
       " 'a',\n",
       " 'great',\n",
       " 'concept',\n",
       " 'but',\n",
       " 'a',\n",
       " 'little',\n",
       " 'rough',\n",
       " 'on',\n",
       " 'the',\n",
       " 'delivery',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'from',\n",
       " 'the',\n",
       " 'incredible',\n",
       " 'food',\n",
       " 'to',\n",
       " 'the',\n",
       " 'warm',\n",
       " 'atmosphere',\n",
       " 'to',\n",
       " 'the',\n",
       " 'friendly',\n",
       " 'service',\n",
       " 'this',\n",
       " 'downtown',\n",
       " 'neighborhood',\n",
       " 'spot',\n",
       " 'doesnt',\n",
       " 'miss',\n",
       " 'a',\n",
       " 'beat',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'great',\n",
       " 'food',\n",
       " 'at',\n",
       " 'reasonable',\n",
       " 'prices',\n",
       " 'makes',\n",
       " 'for',\n",
       " 'an',\n",
       " 'evening',\n",
       " 'that',\n",
       " 'cant',\n",
       " 'be',\n",
       " 'beat',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'look',\n",
       " 'foward',\n",
       " 'to',\n",
       " 'trying',\n",
       " 'the',\n",
       " 'ruben',\n",
       " 'upon',\n",
       " 'returning',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'the',\n",
       " 'only',\n",
       " 'other',\n",
       " 'steakhouse',\n",
       " 'that',\n",
       " 'seems',\n",
       " 'to',\n",
       " 'hold',\n",
       " 'a',\n",
       " 'candle',\n",
       " 'to',\n",
       " 'roths',\n",
       " 'is',\n",
       " 'the',\n",
       " 'strip',\n",
       " 'house',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'we',\n",
       " 'usually',\n",
       " 'go',\n",
       " 'to',\n",
       " 'the',\n",
       " 'chart',\n",
       " 'house',\n",
       " 'to',\n",
       " 'celebrate',\n",
       " 'a',\n",
       " 'birthday',\n",
       " 'or',\n",
       " 'anniversary',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'this',\n",
       " 'little',\n",
       " 'place',\n",
       " 'has',\n",
       " 'a',\n",
       " 'cute',\n",
       " 'interior',\n",
       " 'decor',\n",
       " 'and',\n",
       " 'affordable',\n",
       " 'city',\n",
       " 'prices',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'youll',\n",
       " 'adore',\n",
       " 'it',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'two',\n",
       " 'words',\n",
       " 'free',\n",
       " 'wine',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'the',\n",
       " 'price',\n",
       " 'is',\n",
       " 'reasonable',\n",
       " 'although',\n",
       " 'the',\n",
       " 'service',\n",
       " 'is',\n",
       " 'poor',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'the',\n",
       " 'quantity',\n",
       " 'is',\n",
       " 'also',\n",
       " 'very',\n",
       " 'good',\n",
       " 'you',\n",
       " 'will',\n",
       " 'come',\n",
       " 'out',\n",
       " 'satisfied',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'i',\n",
       " 'have',\n",
       " 'been',\n",
       " 'to',\n",
       " 'roths',\n",
       " 'twice',\n",
       " 'and',\n",
       " 'both',\n",
       " 'times',\n",
       " 'were',\n",
       " 'very',\n",
       " 'disappointing',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'worth',\n",
       " 'visiting',\n",
       " 'the',\n",
       " '1st',\n",
       " 'ave',\n",
       " 'spot',\n",
       " 'because',\n",
       " 'it',\n",
       " 'is',\n",
       " 'the',\n",
       " 'original',\n",
       " 'store',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'i',\n",
       " 'stumbled',\n",
       " 'upon',\n",
       " 'this',\n",
       " 'second',\n",
       " 'floor',\n",
       " 'walkup',\n",
       " 'two',\n",
       " 'fridays',\n",
       " 'ago',\n",
       " 'when',\n",
       " 'i',\n",
       " 'was',\n",
       " 'with',\n",
       " 'two',\n",
       " 'friends',\n",
       " 'in',\n",
       " 'town',\n",
       " 'from',\n",
       " 'la',\n",
       " 'being',\n",
       " 'serious',\n",
       " 'sushi',\n",
       " 'lovers',\n",
       " 'we',\n",
       " 'sat',\n",
       " 'at',\n",
       " 'the',\n",
       " 'sushi',\n",
       " 'bar',\n",
       " 'to',\n",
       " 'be',\n",
       " 'closer',\n",
       " 'to',\n",
       " 'the',\n",
       " 'action',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'ragas',\n",
       " 'is',\n",
       " 'a',\n",
       " 'romantic',\n",
       " 'cozy',\n",
       " 'restaurant',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'the',\n",
       " 'fried',\n",
       " 'rice',\n",
       " 'is',\n",
       " 'amazing',\n",
       " 'here',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'three',\n",
       " 'courses',\n",
       " 'choices',\n",
       " 'include',\n",
       " 'excellent',\n",
       " 'mussels',\n",
       " 'puff',\n",
       " 'pastry',\n",
       " 'goat',\n",
       " 'cheese',\n",
       " 'and',\n",
       " 'salad',\n",
       " 'with',\n",
       " 'a',\n",
       " 'delicious',\n",
       " 'dressing',\n",
       " 'and',\n",
       " 'a',\n",
       " 'hanger',\n",
       " 'steak',\n",
       " 'au',\n",
       " 'poivre',\n",
       " 'that',\n",
       " 'is',\n",
       " 'out',\n",
       " 'of',\n",
       " 'this',\n",
       " 'world',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'its',\n",
       " 'a',\n",
       " 'perfect',\n",
       " 'place',\n",
       " 'to',\n",
       " 'have',\n",
       " 'a',\n",
       " 'amanzing',\n",
       " 'indian',\n",
       " 'food',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'the',\n",
       " 'place',\n",
       " 'is',\n",
       " 'so',\n",
       " 'cool',\n",
       " 'and',\n",
       " 'the',\n",
       " 'service',\n",
       " 'is',\n",
       " 'prompt',\n",
       " 'and',\n",
       " 'curtious',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'at',\n",
       " 'the',\n",
       " 'end',\n",
       " 'youre',\n",
       " 'left',\n",
       " 'with',\n",
       " 'a',\n",
       " 'mild',\n",
       " 'broth',\n",
       " 'with',\n",
       " 'noodles',\n",
       " 'that',\n",
       " 'you',\n",
       " 'can',\n",
       " 'slurp',\n",
       " 'out',\n",
       " 'of',\n",
       " 'a',\n",
       " 'cup',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'i',\n",
       " 'just',\n",
       " 'wonder',\n",
       " 'how',\n",
       " 'you',\n",
       " 'can',\n",
       " 'have',\n",
       " 'such',\n",
       " 'a',\n",
       " 'delicious',\n",
       " 'meal',\n",
       " 'for',\n",
       " 'such',\n",
       " 'little',\n",
       " 'money',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'the',\n",
       " 'food',\n",
       " 'was',\n",
       " 'delicious',\n",
       " 'but',\n",
       " 'do',\n",
       " 'not',\n",
       " 'come',\n",
       " 'here',\n",
       " 'on',\n",
       " 'a',\n",
       " 'empty',\n",
       " 'stomach',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'the',\n",
       " 'wine',\n",
       " 'list',\n",
       " 'is',\n",
       " 'excellent',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'so',\n",
       " 'garantuan',\n",
       " 'that',\n",
       " 'you',\n",
       " 'cant',\n",
       " 'fit',\n",
       " 'them',\n",
       " 'in',\n",
       " 'a',\n",
       " 'toaster',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'ive',\n",
       " 'been',\n",
       " 'to',\n",
       " 'many',\n",
       " 'thai',\n",
       " 'restaurants',\n",
       " 'in',\n",
       " 'manhattan',\n",
       " 'before',\n",
       " 'and',\n",
       " 'toons',\n",
       " 'is',\n",
       " 'by',\n",
       " 'far',\n",
       " 'the',\n",
       " 'best',\n",
       " 'thai',\n",
       " 'food',\n",
       " 'ive',\n",
       " 'had',\n",
       " 'except',\n",
       " 'for',\n",
       " 'my',\n",
       " 'moms',\n",
       " 'of',\n",
       " 'course',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'they',\n",
       " 'wouldnt',\n",
       " 'even',\n",
       " 'let',\n",
       " 'me',\n",
       " 'finish',\n",
       " 'my',\n",
       " 'glass',\n",
       " 'of',\n",
       " 'wine',\n",
       " 'before',\n",
       " 'offering',\n",
       " 'another',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'i',\n",
       " 'highly',\n",
       " 'recommend',\n",
       " 'the',\n",
       " 'sophia',\n",
       " 'pizza',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'whem',\n",
       " 'asked',\n",
       " 'we',\n",
       " 'had',\n",
       " 'to',\n",
       " 'ask',\n",
       " 'more',\n",
       " 'detailed',\n",
       " 'questions',\n",
       " 'so',\n",
       " 'that',\n",
       " 'we',\n",
       " 'knew',\n",
       " 'what',\n",
       " 'the',\n",
       " 'specials',\n",
       " 'were',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'this',\n",
       " 'is',\n",
       " 'a',\n",
       " 'consistently',\n",
       " 'great',\n",
       " 'place',\n",
       " 'to',\n",
       " 'dine',\n",
       " 'for',\n",
       " 'lunch',\n",
       " 'or',\n",
       " 'dinner',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'nice',\n",
       " 'atmosphere',\n",
       " 'the',\n",
       " 'service',\n",
       " 'was',\n",
       " 'very',\n",
       " 'pleasant',\n",
       " 'and',\n",
       " 'the',\n",
       " 'desert',\n",
       " 'was',\n",
       " 'good',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'all',\n",
       " 'the',\n",
       " 'people',\n",
       " 'that',\n",
       " 'i',\n",
       " 'bring',\n",
       " 'there',\n",
       " 'go',\n",
       " 'back',\n",
       " 'on',\n",
       " 'their',\n",
       " 'own',\n",
       " 'and',\n",
       " 'bring',\n",
       " 'their',\n",
       " 'friends',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'after',\n",
       " 'really',\n",
       " 'enjoying',\n",
       " 'ourselves',\n",
       " 'at',\n",
       " 'the',\n",
       " 'bar',\n",
       " 'we',\n",
       " 'sat',\n",
       " 'down',\n",
       " 'at',\n",
       " 'a',\n",
       " 'table',\n",
       " 'and',\n",
       " ...]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(data: pd.DataFrame, col: str, ngram: int,\n",
    "             by_char: bool = False,\n",
    "             sentence_begin: str = SENTENCE_BEGIN,\n",
    "             sentence_end: str = SENTENCE_END):\n",
    "    \"\"\"\n",
    "  Tokenize each line in a list of strings. Glue on the appropriate number of \n",
    "  sentence begin tokens and sentence end tokens (ngram - 1), except\n",
    "  for the case when ngram == 1, when there will be one sentence begin\n",
    "  and one sentence end token.\n",
    "  Args:\n",
    "    data (list): list of strings to tokenize\n",
    "    ngram (int): ngram preparation number\n",
    "    by_char (bool): default value True, if True, tokenize by character, if\n",
    "      False, tokenize by whitespace\n",
    "    sentence_begin (str): sentence begin token value\n",
    "    sentence_end (str): sentence end token value\n",
    "\n",
    "  Returns:\n",
    "    list of strings - all lines tokenized as one large list\n",
    "  \"\"\"\n",
    "    # PROVIDED\n",
    "    total = []\n",
    "    # also glue on sentence begin and end items\n",
    "    for line in df[col]:\n",
    "        line = line.strip()\n",
    "        # skip empty lines\n",
    "        if len(line) == 0:\n",
    "            continue\n",
    "        tokens = tokenize_line(line, ngram, by_char, sentence_begin, sentence_end)\n",
    "        total += tokens\n",
    "    return total\n",
    "tokenize(final_df, 'text', 1)\n",
    "# final_df.sort_values(by=\"text\", key=lambda x: x.str.len(), ascending=False)"
>>>>>>> 4c1e650 (First round of ngram naive bayes (imports broken))
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
=======
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ngrams(tokens: list, n: int) -> list:\n",
    "    \"\"\"Creates n-grams for the given token sequence.\n",
    "  Args:\n",
    "    tokens (list): a list of tokens as strings\n",
    "    n (int): the length of n-grams to create\n",
    "\n",
    "  Returns:\n",
    "    list: list of tuples of strings, each tuple being one of the individual n-grams\n",
    "  \"\"\"\n",
    "    # STUDENTS IMPLEMENT\n",
    "    ngrams = []\n",
    "    for i in range(len(tokens) + 1 - n):\n",
    "        ngrams.append(tuple(tokens[i:i + n]))\n",
    "    return ngrams\n",
    "\n",
    "def is_special(token):\n",
    "    return token == SENTENCE_BEGIN or token == SENTENCE_END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "filter df by food category:\n",
    "\n",
    "ngram model dict will look like:\n",
    "    (ngram tuple): {\n",
    "        positive: val\n",
    "        negative: val\n",
    "        neutral: val\n",
    "        conflict: val\n",
    "    }\n",
    "also count the number of reviews in each class to get the P(class)\n",
    "for example, the probability that 'I like sushi' is positive is\n",
    "P(positive)*(P(<s> | positive))*P(I | positive)*P(like | positive)*P(sushi | positive)*P(</s> | positive)\n",
    "^ for a unigram model\n",
    "\n",
    "probability of <s>  and </s> should be 1,\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "class Ngram_NB:\n",
    "    def __init__(self, n_gram):\n",
    "        \"\"\"Initializes an untrained LanguageModel\n",
    "        Args:\n",
    "          n_gram (int): the n-gram order of the language model to create\n",
    "        \"\"\"\n",
    "        self.n = n_gram\n",
    "        self.category_probabilities = None\n",
    "        self.ngrams = None\n",
    "        self.raw_vocab = None\n",
    "        \n",
    "    def train(self, df, text_col=\"text\", category_col=\"category\", label_col = \"polarity\", verbose: bool = False) -> None:\n",
    "        \"\"\"Trains the language model on the given data. Assumes that the given data\n",
    "        has tokens that are white-space separated, has one sentence per line, and\n",
    "        that the sentences begin with <s> and end with </s>\n",
    "        Args:\n",
    "          tokens (list): tokenized data to be trained on as a single list\n",
    "          verbose (bool): default value False, to be used to turn on/off debugging prints\n",
    "        \"\"\"\n",
    "        # initialize class variables\n",
    "        self.ngrams = dict()\n",
    "        self.category_probabilities = defaultdict(lambda: defaultdict(int))\n",
    "        \n",
    "        # get the list of all tokens to determine rare words\n",
    "        tokens = tokenize(df, text_col, self.n)\n",
    "        self.raw_counts = Counter(tokens)\n",
    "        \n",
    "        # get the list of categories\n",
    "        categories = df[category_col].unique()\n",
    "        \n",
    "        # for each category, filter the rows\n",
    "        for category in categories:\n",
    "            # defaultdict used to easily add elements, initialize to 1 for smoothing purposes\n",
    "            self.ngrams[category] = defaultdict(lambda: defaultdict(lambda: 1)) \n",
    "            category_df = df[df[category_col] == category]\n",
    "            # for each row, tokenize the sentence and create ngrams\n",
    "            # THIS IS SLOW AND CAN PROBABLY BE IMPROVED\n",
    "            for index, row in category_df.iterrows():\n",
    "                sent_tokens = tokenize_line(row[text_col], self.n)\n",
    "                filtered_sent_tokens = self.smooth_tokens(sent_tokens)\n",
    "                ngrams = create_ngrams(filtered_sent_tokens, self.n)\n",
    "                polarity = row['polarity']\n",
    "                \n",
    "                # update class counts for this category\n",
    "                self.category_probabilities[category][polarity] += 1\n",
    "                \n",
    "                # for each ngram in the set of created ngrams, update the ngram counts for this category\n",
    "                for ngram in ngrams:\n",
    "                    # defaultdict should create these entries if they don't exist\n",
    "                    self.ngrams[category][ngram][polarity] += 1\n",
    "        if verbose:\n",
    "            print(self.category_probabilities)\n",
    "   \n",
    "    def smooth_tokens(self, tokens: list):\n",
    "        \"\"\"Smooths a list of tokens by replacing rare words with UNK\n",
    "        \"\"\"\n",
    "        if not self.raw_counts:\n",
    "            raise ValueError(\"Model not yet trained\")\n",
    "        return [token if is_special(token) or self.raw_counts[token] > 1 else UNK for token in tokens]\n",
    "    \n",
    "    def score(self, input_string: str, category: str = 'food', verbose = False) -> float:\n",
    "        \"\"\"Calculates the probability scores for each polarity for a given string representing a single sequence of tokens.\n",
    "        Args:\n",
    "          sentence_tokens (list): a tokenized sequence to be scored by this model\n",
    "\n",
    "        Returns:\n",
    "          str: the most likely class for this string\n",
    "          list: list of tuples, with each tuple containing the polarity and the corresponding\n",
    "        \"\"\"\n",
    "        # STUDENTS IMPLEMENT\n",
    "        if not self.ngrams:\n",
    "            raise ValueError(\"Model not yet trained\")\n",
    "\n",
    "        # default category is food\n",
    "        # COULD CHANGE THIS TO DO SCORE ACROSS ALL CATEGORIES AND CHOOSE THE BEST PROBABILITY\n",
    "        \n",
    "        tokens = tokenize_line(input_string, self.n)\n",
    "\n",
    "        # replace rare words with UNK\n",
    "        smooth_tokens = self.smooth_tokens(tokens)\n",
    "        \n",
    "        ngrams = create_ngrams(smooth_tokens, self.n)\n",
    "        polarities = self.category_probabilities[category].keys()\n",
    "        category_total = sum(self.category_probabilities[category].values())\n",
    "        scores = dict()\n",
    "        \n",
    "        for polarity in polarities:\n",
    "            # probability string is <polarity> = \n",
    "            # P(positive) * PRODUCT(# occurrences of ngram in positive docs / total words in positive docs) for each ngram\n",
    "            score = self.category_probabilities[category][polarity] / category_total # P(<polarity>)\n",
    "            \n",
    "            if verbose:\n",
    "                print(f'probability of {polarity} = {score}')\n",
    "            \n",
    "            # get the total number of ngrams in this polarity\n",
    "            # could calculate this value once during training\n",
    "            polarity_count = 0\n",
    "            for ngram, polarity_counts in self.ngrams[category].items():\n",
    "                polarity_count += polarity_counts[polarity]\n",
    "            \n",
    "            if verbose:\n",
    "                print(f'number of terms with polarity {polarity} = {polarity_count}')\n",
    "            for ngram in ngrams:\n",
    "                ngram_polarity_occurrence = self.ngrams[category][ngram][polarity]\n",
    "                if verbose:\n",
    "                    print(f'number of times {ngram} appeared in polarity = {ngram_polarity_occurrence} / {polarity_count}')\n",
    "                score *= ngram_polarity_occurrence / polarity_count\n",
    "            scores[polarity] = score\n",
    "        label = max(scores, key=scores.get)\n",
    "        return label, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<function Ngram_NB.train.<locals>.<lambda> at 0x0000020A9328FF60>, {'service': defaultdict(<class 'int'>, {'negative': 164, 'positive': 200, 'conflict': 31, 'neutral': 15}), 'food': defaultdict(<class 'int'>, {'positive': 820, 'negative': 187, 'neutral': 86, 'conflict': 64}), 'anecdotes/miscellaneous': defaultdict(<class 'int'>, {'positive': 516, 'neutral': 350, 'negative': 188, 'conflict': 27}), 'ambience': defaultdict(<class 'int'>, {'positive': 143, 'negative': 62, 'conflict': 36, 'neutral': 19}), 'price': defaultdict(<class 'int'>, {'positive': 62, 'negative': 57, 'neutral': 6, 'conflict': 8})})\n",
      "probability of positive = 0.7087294727744166\n",
      "number of terms with polarity positive = 14172\n",
      "number of times ('<s>', 'terrible') appeared in polarity = 1 / 14172\n",
      "number of times ('terrible', 'service') appeared in polarity = 1 / 14172\n",
      "number of times ('service', 'bland') appeared in polarity = 1 / 14172\n",
      "number of times ('bland', 'food') appeared in polarity = 1 / 14172\n",
      "number of times ('food', 'mention') appeared in polarity = 1 / 14172\n",
      "number of times ('mention', '<UNK>') appeared in polarity = 1 / 14172\n",
      "number of times ('<UNK>', '<UNK>') appeared in polarity = 141 / 14172\n",
      "number of times ('<UNK>', 'overall') appeared in polarity = 1 / 14172\n",
      "number of times ('overall', 'experience') appeared in polarity = 1 / 14172\n",
      "number of times ('experience', 'horrible') appeared in polarity = 1 / 14172\n",
      "number of times ('horrible', '</s>') appeared in polarity = 1 / 14172\n",
      "probability of negative = 0.1616248919619706\n",
      "number of terms with polarity negative = 8821\n",
      "number of times ('<s>', 'terrible') appeared in polarity = 2 / 8821\n",
      "number of times ('terrible', 'service') appeared in polarity = 2 / 8821\n",
      "number of times ('service', 'bland') appeared in polarity = 2 / 8821\n",
      "number of times ('bland', 'food') appeared in polarity = 2 / 8821\n",
      "number of times ('food', 'mention') appeared in polarity = 2 / 8821\n",
      "number of times ('mention', '<UNK>') appeared in polarity = 2 / 8821\n",
      "number of times ('<UNK>', '<UNK>') appeared in polarity = 35 / 8821\n",
      "number of times ('<UNK>', 'overall') appeared in polarity = 2 / 8821\n",
      "number of times ('overall', 'experience') appeared in polarity = 2 / 8821\n",
      "number of times ('experience', 'horrible') appeared in polarity = 2 / 8821\n",
      "number of times ('horrible', '</s>') appeared in polarity = 4 / 8821\n",
      "probability of neutral = 0.07433016421780467\n",
      "number of terms with polarity neutral = 7800\n",
      "number of times ('<s>', 'terrible') appeared in polarity = 1 / 7800\n",
      "number of times ('terrible', 'service') appeared in polarity = 1 / 7800\n",
      "number of times ('service', 'bland') appeared in polarity = 1 / 7800\n",
      "number of times ('bland', 'food') appeared in polarity = 1 / 7800\n",
      "number of times ('food', 'mention') appeared in polarity = 1 / 7800\n",
      "number of times ('mention', '<UNK>') appeared in polarity = 1 / 7800\n",
      "number of times ('<UNK>', '<UNK>') appeared in polarity = 14 / 7800\n",
      "number of times ('<UNK>', 'overall') appeared in polarity = 1 / 7800\n",
      "number of times ('overall', 'experience') appeared in polarity = 1 / 7800\n",
      "number of times ('experience', 'horrible') appeared in polarity = 1 / 7800\n",
      "number of times ('horrible', '</s>') appeared in polarity = 1 / 7800\n",
      "probability of conflict = 0.055315471045808126\n",
      "number of terms with polarity conflict = 7811\n",
      "number of times ('<s>', 'terrible') appeared in polarity = 1 / 7811\n",
      "number of times ('terrible', 'service') appeared in polarity = 1 / 7811\n",
      "number of times ('service', 'bland') appeared in polarity = 1 / 7811\n",
      "number of times ('bland', 'food') appeared in polarity = 1 / 7811\n",
      "number of times ('food', 'mention') appeared in polarity = 1 / 7811\n",
      "number of times ('mention', '<UNK>') appeared in polarity = 2 / 7811\n",
      "number of times ('<UNK>', '<UNK>') appeared in polarity = 19 / 7811\n",
      "number of times ('<UNK>', 'overall') appeared in polarity = 1 / 7811\n",
      "number of times ('overall', 'experience') appeared in polarity = 1 / 7811\n",
      "number of times ('experience', 'horrible') appeared in polarity = 1 / 7811\n",
      "number of times ('horrible', '</s>') appeared in polarity = 1 / 7811\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('negative',\n",
       " {'positive': 2.1575309071993123e-44,\n",
       "  'negative': 4.604793947059776e-40,\n",
       "  'neutral': 1.6004882804404917e-43,\n",
       "  'conflict': 3.183150669671307e-43})"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
>>>>>>> 4c1e650 (First round of ngram naive bayes (imports broken))
    "model = Ngram_NB(2)\n",
    "# tokens = tokenize(final_df, 'text', 3)\n",
    "model.train(final_df, verbose=True)\n",
    "\n",
    "model.score('From the terrible service, to the bland food, not to mention the unaccommodating managers, the overall experience was horrible.', verbose = True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
