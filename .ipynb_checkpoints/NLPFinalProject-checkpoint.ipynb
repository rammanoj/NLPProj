{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "llGt_owoTl6G"
   },
   "outputs": [],
   "source": [
    "# global imports\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "# from google.colab import drive\n",
    "import json\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "woU4vDoLZ6Z3",
    "outputId": "6701ad2a-604e-41e1-dd31-5889265d9675"
   },
   "outputs": [],
   "source": [
    "# load the dataset to the colab\n",
    "\n",
    "# drive.mount('/content/drive')\n",
    "# dataset_path = '/content/drive/MyDrive/yelp_dataset/'\n",
    "\n",
    "# reviews = dataset_path + \"yelp_academic_dataset_review.json\"\n",
    "# business = dataset_path + \"yelp_academic_dataset_business.json\"\n",
    "\n",
    "\n",
    "# dataset = dataset_path + \"yelp_data.json\"\n",
    "# data_cleaned = dataset_path + 'yelp_cleaned.json'\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "# contraction_mapping = {\n",
    "#     \"ain't\": \"is not\",\n",
    "#     \"aren't\": \"are not\",\n",
    "#     \"can't\": \"cannot\",\n",
    "#     \"could've\": \"could have\",\n",
    "#     \"couldn't\": \"could not\",\n",
    "#     \"didn't\": \"did not\",\n",
    "#     \"doesn't\": \"does not\",\n",
    "#     \"don't\": \"do not\",\n",
    "#     \"gonna\": \"going to\",\n",
    "#     \"gotta\": \"got to\",\n",
    "#     \"hadn't\": \"had not\",\n",
    "#     \"hasn't\": \"has not\",\n",
    "#     \"haven't\": \"have not\",\n",
    "#     \"he'd\": \"he would\",\n",
    "#     \"he'll\": \"he will\",\n",
    "#     \"he's\": \"he is\",\n",
    "#     \"how'd\": \"how did\",\n",
    "#     \"how'll\": \"how will\",\n",
    "#     \"how's\": \"how is\",\n",
    "#     \"I'd\": \"I would\",\n",
    "#     \"I'll\": \"I will\",\n",
    "#     \"I'm\": \"I am\",\n",
    "#     \"I've\": \"I have\",\n",
    "#     \"isn't\": \"is not\",\n",
    "#     \"it'd\": \"it would\",\n",
    "#     \"it'll\": \"it will\",\n",
    "#     \"it's\": \"it is\",\n",
    "#     \"let's\": \"let us\",\n",
    "#     \"might've\": \"might have\",\n",
    "#     \"must've\": \"must have\",\n",
    "#     \"mustn't\": \"must not\",\n",
    "#     \"shan't\": \"shall not\",\n",
    "#     \"she'd\": \"she would\",\n",
    "#     \"she'll\": \"she will\",\n",
    "#     \"she's\": \"she is\",\n",
    "#     \"should've\": \"should have\",\n",
    "#     \"shouldn't\": \"should not\",\n",
    "#     \"that'll\": \"that will\",\n",
    "#     \"that's\": \"that is\",\n",
    "#     \"there's\": \"there is\",\n",
    "#     \"they'd\": \"they would\",\n",
    "#     \"they'll\": \"they will\",\n",
    "#     \"they're\": \"they are\",\n",
    "#     \"they've\": \"they have\",\n",
    "#     \"wasn't\": \"was not\",\n",
    "#     \"we'd\": \"we would\",\n",
    "#     \"we'll\": \"we will\",\n",
    "#     \"we're\": \"we are\",\n",
    "#     \"we've\": \"we have\",\n",
    "#     \"weren't\": \"were not\",\n",
    "#     \"what'd\": \"what did\",\n",
    "#     \"what'll\": \"what will\",\n",
    "#     \"what're\": \"what are\",\n",
    "#     \"what's\": \"what is\",\n",
    "#     \"what've\": \"what have\",\n",
    "#     \"when's\": \"when is\",\n",
    "#     \"where'd\": \"where did\",\n",
    "#     \"where'll\": \"where will\",\n",
    "#     \"where's\": \"where is\",\n",
    "#     \"who'd\": \"who did\",\n",
    "#     \"who'll\": \"who will\",\n",
    "#     \"who's\": \"who is\",\n",
    "#     \"who've\": \"who have\",\n",
    "#     \"why'd\": \"why did\",\n",
    "#     \"why'll\": \"why will\",\n",
    "#     \"why's\": \"why is\",\n",
    "#     \"won't\": \"will not\",\n",
    "#     \"would've\": \"would have\",\n",
    "#     \"wouldn't\": \"would not\",\n",
    "#     \"you'd\": \"you would\",\n",
    "#     \"you'll\": \"you will\",\n",
    "#     \"you're\": \"you are\",\n",
    "#     \"you've\": \"you have\"\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "yKYaySv0iuME"
   },
   "outputs": [],
   "source": [
    "# **Note:** Only for one-time execution\n",
    "\n",
    "# Condense the dataset to only Restuarents and Food stalls\n",
    "\n",
    "\n",
    "# food_related_items = ['Chicken Wings', 'Bakeries', 'Creperies', 'Asian Fusion', 'Dumplings', 'Chocolatiers & Shops', 'Indian', 'Hot Tub & Pool', 'Ethiopian', 'Hawaiian', 'Poke', 'Soup', 'Vietnamese', 'Waffles', 'Falafel', 'Vegetarian', 'Cheesesteaks', 'Caribbean', 'Pita', 'Sushi Bars', 'Soup', 'Halal', 'Turkish', 'Chinese', 'Japanese Curry', 'Breweries', 'Taiwanese', 'Russian', 'Thai', 'Afghan', 'Tex-Mex', 'Iberian', 'Peruvian', 'Salvadoran', 'Laotian', 'Korean', 'Mexican', 'Dim Sum', 'Hakka', 'Venezuelan', 'Pakistani', 'Malaysian', 'Brazilian', 'Colombian', 'Cajun/Creole', 'Bubble Tea', 'Kebab', 'Trinidadian', 'Cambodian', 'Japanese', 'Tapas Bars', 'Tapas/Small Plates', 'Greek', 'restaurants', 'food']\n",
    "\n",
    "# food_related_business_ids = set()\n",
    "# with open(business, \"r\") as f:\n",
    "#   for i in f:\n",
    "#     json_data = json.loads(i)\n",
    "#     if json_data[\"categories\"] != None:\n",
    "#       for j in food_related_items:\n",
    "#         if j in json_data[\"categories\"]:\n",
    "#           food_related_business_ids.add(json_data[\"business_id\"])\n",
    "\n",
    "\n",
    "# def filter_specific_ids(file_name, out_file_name):\n",
    "#     out_file = open(out_file_name, \"w+\")\n",
    "#     start = True\n",
    "#     with open(file_name, \"r\") as f:\n",
    "#         out_file.write('[')\n",
    "#         for i in f:\n",
    "#             if json.loads(i)[\"business_id\"] in food_related_business_ids:\n",
    "#               if start:\n",
    "#                   start = False\n",
    "#               else:\n",
    "#                   out_file.write(\",\")\n",
    "#               data = json.loads(i)\n",
    "#               del data[\"review_id\"]\n",
    "#               del data[\"user_id\"]\n",
    "#               del data[\"business_id\"]\n",
    "#               del data[\"date\"]\n",
    "#               out_file.write(json.dumps(data))\n",
    "#     out_file.write(']')\n",
    "#     out_file.close()\n",
    "\n",
    "\n",
    "# filter_specific_ids(reviews, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "RHPPmyjfhzPR"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# **Note:** Only for one-time execution\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# cleanse the data\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m dataframe \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_json(dataset)\n\u001b[0;32m      5\u001b[0m stop_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(nltk\u001b[38;5;241m.\u001b[39mcorpus\u001b[38;5;241m.\u001b[39mstopwords\u001b[38;5;241m.\u001b[39mwords(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m      6\u001b[0m lemmatizer \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mstem\u001b[38;5;241m.\u001b[39mWordNetLemmatizer()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# # **Note:** Only for one-time execution\n",
    "\n",
    "# # cleanse the data\n",
    "# dataframe = pd.read_json(dataset)\n",
    "# stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "# lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "# def cleanse_data(d):\n",
    "\n",
    "#   #convert to lower case\n",
    "#   o = d.lower()\n",
    "\n",
    "#   # replace contractions with full forms\n",
    "#   for j in contraction_mapping.keys():\n",
    "#     o = o.replace(j, contraction_mapping[j])\n",
    "\n",
    "#   # replace extra white spaces, special characters\n",
    "#   o = re.sub(r'\\s+', ' ', o)\n",
    "#   o = re.sub(r'[^a-zA-Z0-9\\s]', '', o)\n",
    "\n",
    "#   # remove all new lines\n",
    "#   o = o.replace(\"\\n\", \"\")\n",
    "\n",
    "#   # remove stop words and lemmatize the tokens\n",
    "#   tokens = o.split()\n",
    "#   cleaned_tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
    "#   o = ' '.join(cleaned_tokens)\n",
    "\n",
    "#   # remove punctuation\n",
    "#   o = o.translate(str.maketrans('', '', string.punctuation))\n",
    "#   return o\n",
    "\n",
    "# dataframe[\"text\"] = dataframe[\"text\"].apply(cleanse_data)\n",
    "# dataframe.to_json(data_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "esaM9EX3tzry",
    "outputId": "14b384c9-11e6-4bf9-ea73-dccf24289e3a"
   },
   "outputs": [],
   "source": [
    "# Run to load the dataset\n",
    "\n",
    "json_data = pd.read_json(\"./yelp_cleaned.json\")\n",
    "json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pZkIGKRtCB2p"
   },
   "outputs": [],
   "source": [
    "## Topic modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aspect Extraction\n",
    "# https://www.enjoyalgorithms.com/blog/aspect-base-sentiment-analysis-in-python\n",
    "# run the command below once\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xpath_expression1 = \" //sentence\"\n",
    "xpath_expression2 = \"//sentence/aspectCategories/aspectCategory[1]\"\n",
    "df = pd.read_xml('Restaurants_Train_v2.xml', xpath = xpath_expression1)\n",
    "df2 = pd.read_xml('Restaurants_Train_v2.xml', xpath = xpath_expression2)\n",
    "# Extract category attribute from aspectCategories\n",
    "# df['category'] = df['aspectCategories'].apply(lambda x: x['aspectCategory']['category'])\n",
    "\n",
    "# Drop the aspectCategories column as it's no longer needed\n",
    "# df.drop(columns='aspectCategories', inplace=True)\n",
    "\n",
    "final_df = df.merge(df2, left_index=True, right_index=True)\n",
    "final_df.dropna(axis = 1, inplace=True)\n",
    "categories = final_df['category'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for category in categories:\n",
    "    print(category, len(final_df[final_df['category'] == category]))\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ngram_nb import Ngram_NB \n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Ngram_NB(2)\n",
    "# tokens = tokenize(final_df, 'text', 3)\n",
    "model.train(final_df, verbose=True)\n",
    "\n",
    "model.score('From the terrible service, to the bland food, not to mention the unaccommodating managers, the overall experience was horrible.', verbose = True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
