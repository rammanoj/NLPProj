{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "llGt_owoTl6G"
      },
      "outputs": [],
      "source": [
        "# global imports\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "from google.colab import drive\n",
        "import json\n",
        "import re\n",
        "import nltk\n",
        "import string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "woU4vDoLZ6Z3",
        "outputId": "24205f57-f945-4056-d5a7-4800a62a9d8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# load the dataset to the colab\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "dataset_path = '/content/drive/MyDrive/yelp_dataset/'\n",
        "\n",
        "reviews = dataset_path + \"yelp_academic_dataset_review.json\"\n",
        "business = dataset_path + \"yelp_academic_dataset_business.json\"\n",
        "\n",
        "\n",
        "dataset = dataset_path + \"yelp_data.json\"\n",
        "data_cleaned = dataset_path + 'yelp_clean.json'\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "contraction_mapping = {\n",
        "    \"ain't\": \"is not\",\n",
        "    \"aren't\": \"are not\",\n",
        "    \"can't\": \"cannot\",\n",
        "    \"could've\": \"could have\",\n",
        "    \"couldn't\": \"could not\",\n",
        "    \"didn't\": \"did not\",\n",
        "    \"doesn't\": \"does not\",\n",
        "    \"don't\": \"do not\",\n",
        "    \"gonna\": \"going to\",\n",
        "    \"gotta\": \"got to\",\n",
        "    \"hadn't\": \"had not\",\n",
        "    \"hasn't\": \"has not\",\n",
        "    \"haven't\": \"have not\",\n",
        "    \"he'd\": \"he would\",\n",
        "    \"he'll\": \"he will\",\n",
        "    \"he's\": \"he is\",\n",
        "    \"how'd\": \"how did\",\n",
        "    \"how'll\": \"how will\",\n",
        "    \"how's\": \"how is\",\n",
        "    \"I'd\": \"I would\",\n",
        "    \"I'll\": \"I will\",\n",
        "    \"I'm\": \"I am\",\n",
        "    \"I've\": \"I have\",\n",
        "    \"isn't\": \"is not\",\n",
        "    \"it'd\": \"it would\",\n",
        "    \"it'll\": \"it will\",\n",
        "    \"it's\": \"it is\",\n",
        "    \"let's\": \"let us\",\n",
        "    \"might've\": \"might have\",\n",
        "    \"must've\": \"must have\",\n",
        "    \"mustn't\": \"must not\",\n",
        "    \"shan't\": \"shall not\",\n",
        "    \"she'd\": \"she would\",\n",
        "    \"she'll\": \"she will\",\n",
        "    \"she's\": \"she is\",\n",
        "    \"should've\": \"should have\",\n",
        "    \"shouldn't\": \"should not\",\n",
        "    \"that'll\": \"that will\",\n",
        "    \"that's\": \"that is\",\n",
        "    \"there's\": \"there is\",\n",
        "    \"they'd\": \"they would\",\n",
        "    \"they'll\": \"they will\",\n",
        "    \"they're\": \"they are\",\n",
        "    \"they've\": \"they have\",\n",
        "    \"wasn't\": \"was not\",\n",
        "    \"we'd\": \"we would\",\n",
        "    \"we'll\": \"we will\",\n",
        "    \"we're\": \"we are\",\n",
        "    \"we've\": \"we have\",\n",
        "    \"weren't\": \"were not\",\n",
        "    \"what'd\": \"what did\",\n",
        "    \"what'll\": \"what will\",\n",
        "    \"what're\": \"what are\",\n",
        "    \"what's\": \"what is\",\n",
        "    \"what've\": \"what have\",\n",
        "    \"when's\": \"when is\",\n",
        "    \"where'd\": \"where did\",\n",
        "    \"where'll\": \"where will\",\n",
        "    \"where's\": \"where is\",\n",
        "    \"who'd\": \"who did\",\n",
        "    \"who'll\": \"who will\",\n",
        "    \"who's\": \"who is\",\n",
        "    \"who've\": \"who have\",\n",
        "    \"why'd\": \"why did\",\n",
        "    \"why'll\": \"why will\",\n",
        "    \"why's\": \"why is\",\n",
        "    \"won't\": \"will not\",\n",
        "    \"would've\": \"would have\",\n",
        "    \"wouldn't\": \"would not\",\n",
        "    \"you'd\": \"you would\",\n",
        "    \"you'll\": \"you will\",\n",
        "    \"you're\": \"you are\",\n",
        "    \"you've\": \"you have\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "yKYaySv0iuME"
      },
      "outputs": [],
      "source": [
        "# **Note:** Only for one-time execution\n",
        "\n",
        "# Condense the dataset to only Restuarents and Food stalls\n",
        "\n",
        "\n",
        "food_related_items = ['Chicken Wings', 'Bakeries', 'Creperies', 'Asian Fusion', 'Dumplings', 'Chocolatiers & Shops', 'Indian', 'Hot Tub & Pool', 'Ethiopian', 'Hawaiian', 'Poke', 'Soup', 'Vietnamese', 'Waffles', 'Falafel', 'Vegetarian', 'Cheesesteaks', 'Caribbean', 'Pita', 'Sushi Bars', 'Soup', 'Halal', 'Turkish', 'Chinese', 'Japanese Curry', 'Breweries', 'Taiwanese', 'Russian', 'Thai', 'Afghan', 'Tex-Mex', 'Iberian', 'Peruvian', 'Salvadoran', 'Laotian', 'Korean', 'Mexican', 'Dim Sum', 'Hakka', 'Venezuelan', 'Pakistani', 'Malaysian', 'Brazilian', 'Colombian', 'Cajun/Creole', 'Bubble Tea', 'Kebab', 'Trinidadian', 'Cambodian', 'Japanese', 'Tapas Bars', 'Tapas/Small Plates', 'Greek', 'restaurants', 'food']\n",
        "\n",
        "food_related_business_ids = {}\n",
        "with open(business, \"r\") as f:\n",
        "  for i in f:\n",
        "    json_data = json.loads(i)\n",
        "    if json_data[\"categories\"] != None:\n",
        "      for j in food_related_items:\n",
        "        if j in json_data[\"categories\"]:\n",
        "          food_related_business_ids[json_data[\"business_id\"]] = json_data[\"name\"]\n",
        "\n",
        "\n",
        "def filter_specific_ids(file_name, out_file_name):\n",
        "    out_file = open(out_file_name, \"w+\")\n",
        "    start = True\n",
        "    with open(file_name, \"r\") as f:\n",
        "        out_file.write('[')\n",
        "        for i in f:\n",
        "            if json.loads(i)[\"business_id\"] in food_related_business_ids:\n",
        "              if start:\n",
        "                  start = False\n",
        "              else:\n",
        "                  out_file.write(\",\")\n",
        "              data = json.loads(i)\n",
        "              data[\"business_name\"] = food_related_business_ids[data[\"business_id\"]]\n",
        "              del data[\"review_id\"]\n",
        "              del data[\"user_id\"]\n",
        "              del data[\"business_id\"]\n",
        "              del data[\"date\"]\n",
        "              out_file.write(json.dumps(data))\n",
        "    out_file.write(']')\n",
        "    out_file.close()\n",
        "\n",
        "\n",
        "filter_specific_ids(reviews, dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "RHPPmyjfhzPR"
      },
      "outputs": [],
      "source": [
        "# **Note:** Only for one-time execution\n",
        "\n",
        "# cleanse the data\n",
        "dataframe = pd.read_json(dataset)\n",
        "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
        "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "\n",
        "def cleanse_data(d):\n",
        "\n",
        "  #convert to lower case\n",
        "  o = d.lower()\n",
        "\n",
        "  # replace contractions with full forms\n",
        "  for j in contraction_mapping.keys():\n",
        "    o = o.replace(j, contraction_mapping[j])\n",
        "\n",
        "  # replace extra white spaces, special characters\n",
        "  o = re.sub(r'\\s+', ' ', o)\n",
        "  o = re.sub(r'[^a-zA-Z0-9\\s]', '', o)\n",
        "\n",
        "  # remove all new lines\n",
        "  o = o.replace(\"\\n\", \"\")\n",
        "\n",
        "  # remove stop words and lemmatize the tokens\n",
        "  tokens = o.split()\n",
        "  cleaned_tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
        "  o = ' '.join(cleaned_tokens)\n",
        "\n",
        "  # remove punctuation\n",
        "  o = o.translate(str.maketrans('', '', string.punctuation))\n",
        "  return o\n",
        "\n",
        "dataframe[\"text\"] = dataframe[\"text\"].apply(cleanse_data)\n",
        "dataframe.to_json(data_cleaned)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "esaM9EX3tzry",
        "outputId": "39075654-5e6e-4604-9c08-22b012e263b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         stars  useful  funny  cool  \\\n",
            "0            5       1      0     1   \n",
            "1            1       1      2     1   \n",
            "2            3       0      0     0   \n",
            "3            4       0      2     0   \n",
            "4            5       0      0     0   \n",
            "...        ...     ...    ...   ...   \n",
            "2693380      3       1      0     0   \n",
            "2693381      3       2      0     2   \n",
            "2693382      4       2      0     1   \n",
            "2693383      1       0      0     0   \n",
            "2693384      4       3      0     2   \n",
            "\n",
            "                                                      text  \\\n",
            "0        wow yummy different delicious favorite lamb cu...   \n",
            "1        long term frequent customer establishment went...   \n",
            "2        party 6 hibachi waitress brought separate sush...   \n",
            "3        bun make sonoran dog like snuggie pup first se...   \n",
            "4        tremendous service big shout douglas complemen...   \n",
            "...                                                    ...   \n",
            "2693380  excited food saw unfortunately place close ear...   \n",
            "2693381  later yelp ive love place mainly atmosphere co...   \n",
            "2693382  place never fails food absolutely delicious co...   \n",
            "2693383  average thai food tonight bangkok always go th...   \n",
            "2693384  good maybe good went lunch head chef review ma...   \n",
            "\n",
            "                           business_name  \n",
            "0                                  Zaika  \n",
            "1                               Dmitri's  \n",
            "2        Hibachi Steak House & Sushi Bar  \n",
            "3                               BK Tacos  \n",
            "4                      Rittenhouse Grill  \n",
            "...                                  ...  \n",
            "2693380          Reading Terminal Market  \n",
            "2693381                              Pod  \n",
            "2693382                             Roux  \n",
            "2693383                  Bangkok Cuisine  \n",
            "2693384                          Marhaba  \n",
            "\n",
            "[2693385 rows x 6 columns]\n"
          ]
        }
      ],
      "source": [
        "# Run to load the dataset\n",
        "\n",
        "json_data = pd.read_json(data_cleaned)\n",
        "print(json_data)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Topic modelling"
      ],
      "metadata": {
        "id": "pZkIGKRtCB2p"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}